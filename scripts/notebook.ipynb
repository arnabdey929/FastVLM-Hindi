{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0831233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Torch version 2.6.0 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "import coremltools as ct\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661d696",
   "metadata": {},
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_path = \"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi\"\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "# Wrap encoder (same as before)\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.encoder(input_ids = input_ids)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "# Wrap decoder + lm_head\n",
    "class DecoderWithLMHeadWrapper(torch.nn.Module):\n",
    "    def __init__(self, decoder, lm_head):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states):\n",
    "        outputs = self.decoder(input_ids=input_ids, encoder_hidden_states=encoder_hidden_states)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.lm_head(hidden_states)   # <-- vocab logits\n",
    "        return logits\n",
    "\n",
    "encoder_wrapper = EncoderWrapper(model.model.encoder)\n",
    "decoder_wrapper = DecoderWithLMHeadWrapper(model.model.decoder, model.lm_head)\n",
    "encoder_wrapper.eval()\n",
    "decoder_wrapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945399fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10   # number of tokens\n",
    "vocab_size = 61950\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states = encoder_wrapper(dummy_input)\n",
    "\n",
    "print(\"Dummy input to encoder:\", dummy_input)\n",
    "print(\"Hidden states shape:\", hidden_states.shape)\n",
    "print(\"Hidden states sample:\", hidden_states[0, 0, :10])  # print first 10 dims of first token\n",
    "\n",
    "start_token_id = 61949\n",
    "decoder_input_ids = torch.tensor([[start_token_id]])  # shape: [1, 1]\n",
    "\n",
    "# 4️⃣ Forward pass through decoder\n",
    "with torch.no_grad():\n",
    "    logits = decoder_wrapper(decoder_input_ids, hidden_states)\n",
    "print(\"Logits output from decoder:\", logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd5ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "/opt/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:81: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Converting PyTorch Frontend ==> MIL Ops:   9%|▉         | 20/228 [00:00<00:01, 126.94 ops/s]Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 227/228 [00:00<00:00, 557.62 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 180.57 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]/opt/miniconda3/envs/py310/lib/python3.10/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '396', of the source model, has been renamed to 'var_396' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:02<00:00, 34.35 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 279.73 passes/s]\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "max_src_len = 50\n",
    "dummy_encoder_input = torch.ones((1, max_src_len), dtype=torch.long)\n",
    "\n",
    "# ---- Encoder ----\n",
    "encoder_traced = torch.jit.trace(encoder_wrapper, dummy_encoder_input)\n",
    "encoder_mlmodel = ct.convert(\n",
    "    encoder_traced,\n",
    "    inputs=[ct.TensorType(name=\"input_ids\", shape=(1, ct.RangeDim(1, max_src_len)))],\n",
    "    compute_units=ct.ComputeUnit.ALL,\n",
    ")\n",
    "encoder_mlmodel.save(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/opus-mt-en-hi Converted/MarianEncoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Decoder ----\n",
    "max_tgt_len = 50\n",
    "hidden_size = model.config.d_model\n",
    "\n",
    "decoder_input_ids = torch.ones((1, max_tgt_len), dtype=torch.long)   # decoding 5 tokens\n",
    "encoder_hidden_states = torch.ones((1, max_src_len, hidden_size), dtype=torch.float32)\n",
    "\n",
    "decoder_traced = torch.jit.trace(decoder_wrapper, (decoder_input_ids, encoder_hidden_states))\n",
    "decoder_mlmodel = ct.convert(\n",
    "    decoder_traced,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"decoder_input_ids\", shape=(1, ct.RangeDim(1, max_tgt_len))),\n",
    "        ct.TensorType(name=\"encoder_hidden_states\", shape=(1, ct.RangeDim(1, max_src_len), hidden_size)),\n",
    "        # ct.TensorType(name=\"decoder_input_ids\", shape=(1, 50)),\n",
    "        # ct.TensorType(name=\"encoder_hidden_states\", shape=(1, 50, hidden_size)),\n",
    "    ],\n",
    "    compute_units=ct.ComputeUnit.ALL,\n",
    ")\n",
    "decoder_mlmodel.save(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/opus-mt-en-hi Converted/MarianDecoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12697c45",
   "metadata": {},
   "source": [
    "# Checking the exported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ceea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from coremltools.models import MLModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cee8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ct.models.MLModel(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/opus-mt-en-hi Converted/MarianEncoder.mlpackage\")\n",
    "decoder = ct.models.MLModel(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/opus-mt-en-hi Converted/MarianDecoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = np.ones((1, 50), dtype=np.float32)\n",
    "encoder_hidden = np.random.randn(1, 50, 512).astype(np.float32)\n",
    "\n",
    "decoder_input_array = ct.TensorType(shape=decoder_input.shape, dtype=np.float32)\n",
    "encoder_hidden_array = ct.TensorType(shape=encoder_hidden.shape, dtype=np.float32)\n",
    "\n",
    "out = decoder.predict({\n",
    "    \"decoder_input_ids\": decoder_input_array,\n",
    "    \"encoder_hidden_states\": encoder_hidden_array\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707f61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [244, 23, 4, 8496, 765, 3, 0]  # your encoded token IDs\n",
    "tokens_array = np.array(tokens, dtype=np.float32).reshape(1, -1)  # shape [1, seq_len]\n",
    "\n",
    "# For decoder, first token is <pad>\n",
    "start_token = np.array([61949], dtype=np.float32).reshape(1, 1)  # shape [1,1]encoder.predict([\"input_ids\": tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23140a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (1, 7, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"/Users/arnabdey/Library/Caches/python/com.apple.e5rt.e5bundlecache/24E263/8E3EADF9EA3EBC25650E1BA07543095B9FCADF437A4BAB9CE1E6B263A84A8E3B/02E24F99A828CE575EB4193AA01D5B26C2D6948FDBDD178FA46746899EA5BFBE.bundle/H14G.bundle/main/main_mps_graph/main_mps_graph.mpsgraphpackage/model_0.mpsgraph\":0:0): error: attempting to parse a byte at the end of the bytecode\n"
     ]
    }
   ],
   "source": [
    "encoder_input = {\"input_ids\": tokens_array}\n",
    "encoder_output = encoder.predict(encoder_input)\n",
    "\n",
    "# Usually output key is 'var_396' or check encoder.output_description\n",
    "hidden_states = encoder_output[\"var_396\"]\n",
    "print(\"Encoder output shape:\", hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8600c1",
   "metadata": {},
   "source": [
    "## Convert Newly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4cdf2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 128\n",
    "hidden_dim = 512\n",
    "vocab_size = 61950\n",
    "\n",
    "dummy_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "dummy_encoder_hidden = torch.randn(batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36bf85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/228 [00:00<?, ? ops/s]Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 227/228 [00:00<00:00, 513.39 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 122.24 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 90.98 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 283.27 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MarianEncoder.mlpackage\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "encoder_wrapper.eval()\n",
    "\n",
    "traced_encoder = torch.jit.trace(encoder_wrapper, dummy_input_ids)\n",
    "\n",
    "encoder_model = ct.convert(\n",
    "    traced_encoder,\n",
    "    inputs=[ct.TensorType(name=\"input_ids\", shape=(1, ct.RangeDim(1, 128)), dtype=np.int32)],\n",
    "    convert_to=\"mlprogram\",\n",
    "    minimum_deployment_target=ct.target.iOS17,\n",
    "    compute_precision=ct.precision.FLOAT32\n",
    ")\n",
    "\n",
    "encoder_model.save(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/Converted MLPackages (new)/MarianEncoder.mlpackage\")\n",
    "print(\"Saved MarianEncoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f579187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/561 [00:00<?, ? ops/s]Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 560/561 [00:00<00:00, 3695.71 ops/s]\n",
      "Running MIL frontend_pytorch pipeline:   0%|          | 0/5 [00:00<?, ? passes/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 81.33 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:01<00:00, 75.42 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 139.17 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MarianDecoder.mlpackage\n"
     ]
    }
   ],
   "source": [
    "decoder_wrapper.eval()\n",
    "\n",
    "traced_decoder = torch.jit.trace(decoder_wrapper, (dummy_input_ids, dummy_encoder_hidden))\n",
    "\n",
    "decoder_model = ct.convert(\n",
    "    traced_decoder,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"decoder_input_ids\", shape=(1, 128), dtype=np.int32),\n",
    "        ct.TensorType(name=\"encoder_hidden_states\", shape=(1, 128, 512), dtype=np.float32)\n",
    "    ],\n",
    "    convert_to=\"mlprogram\",\n",
    "    minimum_deployment_target=ct.target.iOS17,\n",
    "    compute_precision=ct.precision.FLOAT32\n",
    ")\n",
    "\n",
    "decoder_model.save(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/Converted MLPackages (new)/MarianDecoder.mlpackage\")\n",
    "print(\"Saved MarianDecoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdb495",
   "metadata": {},
   "source": [
    "Test the converted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7876230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_encoder = ct.models.MLModel(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/Converted MLPackages (new)/MarianEncoder.mlpackage\")\n",
    "ml_decoder = ct.models.MLModel(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Convert to coreML/Converted MLPackages (new)/MarianDecoder.mlpackage\")\n",
    "\n",
    "with open(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86e0a8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_396 <class 'numpy.ndarray'> (1, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"/Users/arnabdey/Library/Caches/python/com.apple.e5rt.e5bundlecache/24E263/36B4862F069ADC74E420276BE9921F6ADE8E6493594C4AF2DCCF663F5AD0DC30/DE31E3B0A783900602BBA4774C406682DAEE61A582245BAA721ACD5675F4BFDE.bundle/H14G.bundle/main/main_mps_graph/main_mps_graph.mpsgraphpackage/model_0.mpsgraph\":0:0): error: attempting to parse a byte at the end of the bytecode\n"
     ]
    }
   ],
   "source": [
    "enc_input = dummy_input_ids.numpy().astype(np.int32)\n",
    "\n",
    "enc_out = ml_encoder.predict({\"input_ids\": enc_input})\n",
    "\n",
    "for k, v in enc_out.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = dummy_input_ids.numpy().astype(np.int32)\n",
    "encoder_hidden = enc_out[\"var_396\"].astype(np.float32)\n",
    "\n",
    "dec_out = ml_decoder.predict({\n",
    "    \"decoder_input_ids\": dec_input,\n",
    "    \"encoder_hidden_states\": encoder_hidden\n",
    "})\n",
    "\n",
    "for k, v in dec_out.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dc4f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 61950)\n"
     ]
    }
   ],
   "source": [
    "print(dec_out['var_829'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa8a3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"/Users/arnabdey/Library/Caches/python/com.apple.e5rt.e5bundlecache/24E263/9AD997F09B45F6F92BC492ECFAE7C92D25FDCA1C670D6F6C037BE1ECAAB7B15A/C690509FC1C5576073E7FDD04FE200A05BF8DC8138A64145CF0B044236E8FED1.bundle/H14G.bundle/main/main_mps_graph/main_mps_graph.mpsgraphpackage/model_0.mpsgraph\":0:0): error: attempting to parse a byte at the end of the bytecode\n"
     ]
    }
   ],
   "source": [
    "text = \"I am a man with a big dick.\"\n",
    "decoder_start_token = np.array([[61949 for _ in range(128)]], dtype=np.float32)\n",
    "tokenizer = MarianTokenizer(source_spm=\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi/source.spm\", target_spm=\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi/target.spm\", vocab=\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi/vocab.json\")\n",
    "\n",
    "enc_input = np.array([tokenizer.encode(text)], dtype=np.float32)\n",
    "# print(enc_input)\n",
    "\n",
    "enc_out = ml_encoder.predict({\"input_ids\": enc_input})\n",
    "encoder_hidden = np.pad(enc_out['var_396'], ((0, 0), (0, 128 - enc_input.shape[1]), (0, 0)), mode='constant', constant_values=0)\n",
    "# print(encoder_hidden.shape)\n",
    "dec_out = ml_decoder.predict({\"decoder_input_ids\": decoder_start_token, \"encoder_hidden_states\": np.array(encoder_hidden, dtype=np.float16)})\n",
    "last_token_logits = np.exp(dec_out['var_829'][:, -1, :])\n",
    "probabilities = last_token_logits[0] / np.sum(last_token_logits)\n",
    "# print(dec_out['var_829'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a068b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁मैं\n",
      "▁एक\n",
      "▁बड़े\n",
      "▁डिक\n",
      "▁के\n",
      "▁साथ\n",
      "▁एक\n",
      "▁आदमी\n",
      "▁हूँ\n",
      "।\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "output_till_now = [61949]\n",
    "\n",
    "for i in range(20):\n",
    "    decoder_inputs = np.array([output_till_now + [61949 for _ in range(128 - len(output_till_now))]], dtype=np.float32)\n",
    "    dec_out = ml_decoder.predict({\"decoder_input_ids\": np.array(decoder_inputs, dtype=np.int32), \"encoder_hidden_states\": np.array(encoder_hidden, dtype=np.float16)})\n",
    "    last_token_logits = np.exp(dec_out['var_829'][:, i, :])\n",
    "    probabilities = last_token_logits[0] / np.sum(last_token_logits)\n",
    "    index = np.argmax(probabilities)\n",
    "    print(id_to_token[index])\n",
    "    output_till_now.append(index)\n",
    "    if(id_to_token[index] == '</s>'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fd4dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.440e+02 2.300e+01 4.000e+00 8.496e+03 7.650e+02 2.200e+01 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(enc_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9dcbf7",
   "metadata": {},
   "source": [
    "# Get Start Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c611e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/arnabdey/Documents/MyCodes/Python Folders/Personal Projects/FastVLM/Models/opus-mt-en-hi/vocab.json\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291e603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁spades: 2939\n",
      "pad: 14586\n",
      "▁padded: 31332\n",
      "▁Pad: 34687\n",
      "▁pads: 41055\n",
      "▁padding: 42866\n",
      "▁Cappadocia: 45741\n",
      "▁Padding: 51723\n",
      "▁Launchpad: 57832\n",
      "▁Paddan: 59225\n",
      "<pad>: 61949\n"
     ]
    }
   ],
   "source": [
    "matches = {k: v for k, v in vocab.items() if \"pad\" in k.lower()}\n",
    "\n",
    "if matches:\n",
    "    for token, idx in matches.items():\n",
    "        print(f\"{token}: {idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
